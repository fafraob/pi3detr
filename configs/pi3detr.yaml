### Training parameters
epochs: 1715
lr_step: 1250
lr_warmup_epochs: 15
lr_warmup_start_factor: 1.0e-6
lr: 1.0e-4
batch_size: 8
batch_size_val: 8
accumulate_grad_batches: 16
grad_clip_val: 0.2  # max gradient norm
to_monitor: "val_seg_iou"
monitor_mode: "max"
val_interval: 1

### Model parameters
model: "pi3detr"
preencoder_type: "samodule"
num_features: 0
weights: ""
preencoder_lr: 1.0e-4
freeze_backbone: false
encoder_dim: 768
decoder_dim: 768
num_encoder_layers: 3
num_decoder_layers: 9
encoder_dropout: 0.1  # dropout in encoder
decoder_dropout: 0.1  # dropout in decoder
num_attn_heads: 8  # number of attention heads
enc_dim_feedforward: 2048  # dimension of feedforward in encoder
dec_dim_feedforward: 2048  # dimension of feedforward in decoder
mlp_dropout: 0.0  # dropout in MLP heads
num_preds: 128  # num outputs of transformer
num_classes: 5
auxiliary_loss: true
max_points_in_param: 4
num_transformer_points: 2048  # number of transformer points (needed for some preencoders)
query_type: "point_fps"
pos_embed_type: "sine"  # Options: "fourier", "sine"
class_loss_type: "cross_entropy"
class_loss_weights: [0.04834912,  0.40329467, 0.09588135, 0.23071379, 0.22176106]

### Curve and validation parameters
num_curve_points: 64  # must be same as points_per_curve
num_curve_points_val: 256  # validation curve points

### Loss weights
loss_weights:
  loss_class: 1
  loss_bspline: 1
  loss_bspline_chamfer: 1
  loss_line_position: 1
  loss_line_length: 1
  loss_line_chamfer: 1
  loss_circle_position: 1
  loss_circle_radius: 1
  loss_circle_chamfer: 1
  loss_arc: 1
  loss_arc_chamfer: 1
  loss_seg: 1

### Cost weights
cost_weights:
  cost_class: 1
  cost_curve: 1

### Dataset parameters
dataset: "abc_dataset"
num_workers: 8
data_root: "/dataset/train"
data_val_root: "/dataset/val"
data_test_root: "/dataset/test"
augment: true
random_rotate_prob: 1.0
random_sample_prob: 0.85
random_sample_bounds: [1.0, 0.2]  # [max, min] fraction of points to keep
noise_prob: 0.0
noise_scale: 0.0